{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from keras import applications\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras import backend as k \n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "k.set_image_dim_ordering('tf')\n",
    "img_width = 64\n",
    "img_height = 64\n",
    "train_data_dir = \"data/train\"\n",
    "validation_data_dir = \"data/valid\"\n",
    "nb_train_samples = 25\n",
    "nb_validation_samples = 8\n",
    "batch_size = 1\n",
    "epochs = 50\n",
    "num_of_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading base model.\n",
    "model = VGG16(weights='imagenet', include_top=False,input_shape = (img_width, img_height,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[:-5]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADDING FC LAYERS ON TOP OF VGG16..\n",
    "top_layers = model.output\n",
    "top_layers = Flatten(input_shape=model.output_shape[1:])(top_layers)\n",
    "top_layers = Dense(128, activation=\"relu\",input_shape=(num_of_classes,))(top_layers)\n",
    "top_layers = Dropout(0.2)(top_layers)\n",
    "top_layers = Dense(64, activation=\"relu\",input_shape=(num_of_classes,))(top_layers)\n",
    "top_layers = Dropout(0.2)(top_layers)\n",
    "top_layers = Dense(num_of_classes, activation=\"relu\",input_shape=(num_of_classes,))(top_layers)\n",
    "top_layers = Dense(num_of_classes, activation=\"softmax\")(top_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final = Model(input = model.input, output = top_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final.compile(loss = \"binary_crossentropy\", optimizer = optimizers.SGD(lr=0.0001, momentum=0.9), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize test and training data\n",
    "train_datagen = ImageDataGenerator(\n",
    "rescale = 1./255,\n",
    "horizontal_flip = True,\n",
    "fill_mode = \"nearest\",\n",
    "zoom_range = 0.3,\n",
    "width_shift_range = 0.3,\n",
    "height_shift_range=0.3,\n",
    "rotation_range=30)\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "rescale = 1./255,\n",
    "horizontal_flip = True,\n",
    "fill_mode = \"nearest\",\n",
    "zoom_range = 0.3,\n",
    "width_shift_range = 0.3,\n",
    "height_shift_range=0.3,\n",
    "rotation_range=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27 images belonging to 2 classes.\n",
      "Found 6 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "train_data_dir,\n",
    "target_size = (img_height, img_width),\n",
    "batch_size = batch_size, \n",
    "class_mode = \"categorical\")\n",
    "\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "validation_data_dir, target_size = (img_height, img_width), class_mode = \"categorical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"vgg16_12.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.5329 - acc: 0.8333\n",
      "Epoch 00001: val_acc did not improve\n",
      "25/25 [==============================] - 11s 440ms/step - loss: 0.5382 - acc: 0.8400 - val_loss: 0.6791 - val_acc: 0.5833\n",
      "Epoch 2/10\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.5895 - acc: 0.6250\n",
      "Epoch 00002: val_acc did not improve\n",
      "25/25 [==============================] - 9s 367ms/step - loss: 0.5695 - acc: 0.6400 - val_loss: 0.5098 - val_acc: 0.8333\n",
      "Epoch 3/10\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.6430 - acc: 0.7500\n",
      "Epoch 00003: val_acc did not improve\n",
      "25/25 [==============================] - 9s 367ms/step - loss: 0.6437 - acc: 0.7600 - val_loss: 0.5513 - val_acc: 0.8125\n",
      "Epoch 4/10\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.5229 - acc: 0.8333\n",
      "Epoch 00004: val_acc did not improve\n",
      "25/25 [==============================] - 9s 366ms/step - loss: 0.5297 - acc: 0.8400 - val_loss: 0.5354 - val_acc: 0.8333\n",
      "Epoch 5/10\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.5206 - acc: 0.7083\n",
      "Epoch 00005: val_acc did not improve\n",
      "25/25 [==============================] - 9s 373ms/step - loss: 0.4998 - acc: 0.7200 - val_loss: 0.5341 - val_acc: 0.8333\n",
      "Epoch 6/10\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.5312 - acc: 0.8333\n",
      "Epoch 00006: val_acc improved from 0.89062 to 0.97917, saving model to vgg16_12.h5\n",
      "25/25 [==============================] - 12s 464ms/step - loss: 0.5104 - acc: 0.8400 - val_loss: 0.4004 - val_acc: 0.9792\n",
      "Epoch 7/10\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.5879 - acc: 0.7917\n",
      "Epoch 00007: val_acc did not improve\n",
      "25/25 [==============================] - 10s 403ms/step - loss: 0.5902 - acc: 0.8000 - val_loss: 0.6436 - val_acc: 0.6875\n",
      "Epoch 8/10\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.4490 - acc: 0.7917\n",
      "Epoch 00008: val_acc did not improve\n",
      "25/25 [==============================] - 10s 383ms/step - loss: 0.4700 - acc: 0.7600 - val_loss: 0.4924 - val_acc: 0.8750\n",
      "Epoch 9/10\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.4857 - acc: 0.8750\n",
      "Epoch 00009: val_acc did not improve\n",
      "25/25 [==============================] - 9s 371ms/step - loss: 0.4918 - acc: 0.8800 - val_loss: 0.5503 - val_acc: 0.8333\n",
      "Epoch 10/10\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.4711 - acc: 0.8333\n",
      "Epoch 00010: val_acc did not improve\n",
      "25/25 [==============================] - 9s 371ms/step - loss: 0.4887 - acc: 0.8000 - val_loss: 0.5892 - val_acc: 0.7292\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26e960b24e0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_final.fit_generator(\n",
    "train_generator,\n",
    "samples_per_epoch = nb_train_samples,\n",
    "epochs = epochs,\n",
    "validation_data = validation_generator,\n",
    "nb_val_samples = nb_validation_samples,\n",
    "callbacks = [checkpoint, early]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, Training is done. Now, let's do testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path1 = \"data/test/not\"\n",
    "test_path2 = \"data/test/sure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nt1.jpg', 'nt2.jpg', 'nt3.jpg', 'nt4.jpg'] ['st1.jpg', 'st2.jpg', 'st3.jpg', 'st4.jpg']\n"
     ]
    }
   ],
   "source": [
    "no = [f for f in listdir(test_path1) if isfile(join(test_path1, f))]\n",
    "sure = [f for f in listdir(test_path2) if isfile(join(test_path2, f))]\n",
    "print(no, sure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not\n",
      "not\n",
      "not\n",
      "not\n"
     ]
    }
   ],
   "source": [
    "for i in no:\n",
    "    test_image = image.load_img(test_path1 + \"/\" + i, target_size = (64, 64))\n",
    "    test_image = image.img_to_array(test_image)\n",
    "    test_image = np.expand_dims(test_image, axis = 0)\n",
    "    result = model_final.predict(test_image)\n",
    "    if result[0][0] == 1:\n",
    "        prediction = 'not'\n",
    "    else:\n",
    "        prediction = 'sure'\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Whoa, 100% Accurate on TEST DATA on \"not torn\" shirts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not\n",
      "sure\n",
      "sure\n",
      "sure\n"
     ]
    }
   ],
   "source": [
    "for i in sure:\n",
    "    test_image = image.load_img(test_path2 + \"/\" + i, target_size = (64, 64))\n",
    "    test_image = image.img_to_array(test_image)\n",
    "    test_image = np.expand_dims(test_image, axis = 0)\n",
    "    result = model_final.predict(test_image)\n",
    "    if result[0][0] == 1:\n",
    "        prediction = 'not'\n",
    "    else:\n",
    "        prediction = 'sure'\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 75% On \"torn shirts\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSO, OVERALL, BY LESS THAN 50 IMAGES OF TRAINING, MODEL PERFORMS GOOD.\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "SO, OVERALL, BY LESS THAN 50 IMAGES OF TRAINING, MODEL PERFORMS GOOD.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
